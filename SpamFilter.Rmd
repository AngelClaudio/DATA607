---
title: "Spam Filters"
author: "Angel Claudio"
date: "April 2020"
output: rmdformats::material
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tm)
library(tidyverse)
library(tidytext) 
library(magrittr)
library(plyr)
library(e1071)
library(caret)
library(kernlab)
library(doMC)
library(parallel)
library(doParallel)


```

# Abstract

This research article is about creating Spam Filters using different machine learning models for training and testing. We will see most of the challenges will come from both prepping the data and optimizing the performance of the training. Finally, we will assess the prediction results for highest performance based on accuracy and time efficiency.

# Data Wrangling {.tabset}

We will begin by importing the data, analyzing it, and then performing any data munging needed before applying spam filters.

## Import with Text Mining Package

We will begin with importing the data from our local directory. The package we will be using is the **tm** library (short for Text Mining Package). The following two functions we will use are:

1. **DirSource** which accepts a character vector of full path name that correspond to the working directory (good time to use **setwd** to prep beforehand)

2. **VCorpus** which stores a collection of documents in memory (hence V for volatile)


```{r import-data}
ds_ham <- VCorpus(DirSource("./ham"))
ds_spam <- VCorpus(DirSource("./spam"))
```

We can see below that there is roughly 500 plus documents in each corpus.

```{r inspect-data}
ds_ham
ds_spam
```

## Data Analysis

A __Document Term Matrix__ is simply a matrix that describes the frequency of terms that occur. 

Using the **DocumentTermMatrix** function from the **tm** package we will extract the first 10 document from each corpus.

```{r doctermmatrix}
dtm_ham <- DocumentTermMatrix(ds_ham[1:10])
dtm_spam <- DocumentTermMatrix(ds_spam[1:10])
```


We can use the **inspect** function to take a look at the document term matrix created for each corpus. Below we can see a summary for each.

**Sparsity** refers to the threshold of the relative document frequency for a term, **above** which the term will be removed.


```{r}
inspect(dtm_ham)
inspect(dtm_spam)
```

## Cleaning Up the Data

From the previous section we saw that upon inspection of the data it was quite dirty with punctuation and other terms we probably were not interested in such as numbers.

The **tm** text mining package comes with a handy tool called **tm_map**, which applies transformations to documents. The transformations that are applied are functions such as **removeNumbers**, again all conveniently found as part of the **tm** library.

```{r data-clean}
ds_ham %<>% tm_map(content_transformer(tolower)) %>% 
    tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% 
    tm_map(removeWords, stopwords()) %>% tm_map(stemDocument) %>% 
    tm_map(removeNumbers)

ds_spam %<>% tm_map(content_transformer(tolower)) %>% 
    tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% 
    tm_map(removeWords, stopwords()) %>% tm_map(stemDocument) %>%
    tm_map(removeNumbers)
```

We can now re-inspect to verify that each corpus has been cleaned up

```{r reexam-data}
dtm_ham <- DocumentTermMatrix(ds_ham[1:10])
dtm_spam <- DocumentTermMatrix(ds_spam[1:10])
inspect(dtm_ham)
inspect(dtm_spam)
```

## Final Transformations

Now that we verified the data has been cleaned up, we will perform some final transformations to prepare the data for spam filters.

In order to not waste memory we overwrite existing variables with the entire corpus of each dataset. To optimize performance we will apply sparsity to terms that don't even appear in at least 5% of documents.

We make sure to classify using 0 as ham and 1 as spam, since as a convention 1 is usually used for raising alarms or flags. 

```{r final-transformations}
ham <- DocumentTermMatrix(ds_ham) %>% removeSparseTerms(.95) %>% as.matrix() %>%
    cbind("IsSpam" = 0)

spam <- DocumentTermMatrix(ds_spam) %>% removeSparseTerms(.95) %>% as.matrix() %>%
    cbind("IsSpam" = 1)
```


The classification should be done over a factor type, so we use **as.factor** to ensure the correct data type. Lastly we combine both data sets using the **rbind.fill.matrix** from the **plyr** package and put it in it's final container as a data frame using **as.data.frame**. We validate any **NA** cohersions by setting NA values to 0.

```{r}
all_emails <- rbind.fill.matrix(ham, spam) %>% as.data.frame() %>% 
    transform(IsSpam = as.factor(IsSpam))

all_emails[is.na(all_emails)] <- 0
```

# Spam Filter Approaches {.tabset}

To develop the Spam Filters we start by creating the models and training them. In the final part we will fit the models to testing data to see how well they perform.

## System Prep for Model Creation

R by default only uses a single thread, which can make training models extremely slow. In order to speed up R's performance we can use the libraries **parallel** and **doparallel** to take advantage of parallel processing.

When using the **train** function of the **caret** package, we need to set the argument of **trControl** by passing a **trainControl** object. This object specifies the **number** of folds used for k-fold cross validation. It also tells **caret** to use the **cluster** registered in **registerDoParallel** when assigning the value **TRUE** to the **allowParallel** argument.

The below step is quite tedious, but the amount of time it saves is invaluable as we will see from our timers.

```{r cluster-setup}
cluster <- makeCluster(detectCores() - 1)

registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```

Next we want to randomize the sampling, and divide the sample into a set for training and setting.

Using the **set.seed** function, we're able to create reproducible randomization. All we need to do is initialize (set the see) to any integer value. In other words we are saying whatever random generator you use, start the algorithm using this value. This results in being able to reproduce randomization (confusing since that in itself seems like an oxymoron).

```{r seed-set}
set.seed(315429)
```

Now that seed has been set we will divide the data set into two groups, one for training and one for testing the model. Since we have a pretty good volume, an 80-20 split will work well.

Lastly we first create a vector of indexes created from a "random" (remember we are using a set seed) sampling of 80% of all the emails. Using that index we subset our training and testing data.

```{r split-sets}
training_emails <- sample(1:nrow(all_emails), 0.8 * nrow(all_emails))

all_emails_training <- all_emails[training_emails, ]

all_emails_test <- all_emails[-training_emails, ]
```


## Creating the Machine Learning Models

### Generalized Linear Model Creation

Using the **caret** library we can leverage the **train** function for reusability. We must set the argument **method** to specify what type of model we will be using. It is also very important to set the **trControl** argument as mentioned previously so that training is optimized with parallel processing

```{r glm-create-model, warning=F}
start_time <- Sys.time()

gl_model <- train(IsSpam ~ ., data = all_emails_training, method = 'glm',
                        trControl = fitControl)

Sys.time() - start_time
```

### Support Vector Machine with Linear Kernel Model Creation

```{r svm-create-model, warning=F}
start_time <- Sys.time()

svm_model <- train(IsSpam ~ ., data = all_emails_training, method = 'svmLinear2',
                        trControl = fitControl)

Sys.time() - start_time
```

### Bayes GLM Model Creation

```{r bayesian-create-model, warning=F}
start_time <- Sys.time()

bayes_model <- train(IsSpam ~ ., data = all_emails_training, method = 'bayesglm',
                        trControl = fitControl)

Sys.time() - start_time
```


### Random Forest Model Creation

```{r ranger-create-model, warning=F}
start_time <- Sys.time()

ranger_model <- train(IsSpam ~ ., data = all_emails_training, method = 'ranger', 
                     trControl = fitControl)

Sys.time() - start_time
```

# Model Fitting {.tabset}

From the **stats** library we will use the **predict** function by passing any of our models to the **object** argument. Finally, we will use the **confusionMatrix** function from the **caret** library to assess the actual performance for each model.

## GLM Prediction

```{r glm-predict, warning=F}
predict(gl_model, newdata = all_emails_test) %>% 
    confusionMatrix(all_emails_test$IsSpam)
```

## SVM Prediction

```{r svm-predict, warning=F}
predict(svm_model, newdata = all_emails_test) %>% 
    confusionMatrix(all_emails_test$IsSpam)
```

## Bayes GLM Prediction

```{r bay-predict, warning=F}
predict(bayes_model, newdata = all_emails_test) %>% 
    confusionMatrix(all_emails_test$IsSpam)
```

## Random Forest Prediction

```{r ranger-predict, warning=F}
predict(ranger_model, newdata = all_emails_test) %>% 
    confusionMatrix(all_emails_test$IsSpam)
```


# Conclusion

We saw that the majority of work came from the data munging process and the training of the models. We were very generous with the volume of data because we wanted to gain as much accuracy as possible. However, in the beginning of this research the performance impact for using such volume was over 10 minutes sometimes to train individual models. The big turning point in the research was enabling parallel processing which brought down the training time for each model to mostly an impressive sub-minute for each. Overall the General Linear Model had the poorest performance while the rest performed perfectly with varying completion times. The Supported Vector Machine with Linear Kernel Model performed the quickest out of all 4 models, about 1/20th of the longest time.
